{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51bb4e149ab101b9",
   "metadata": {},
   "source": [
    "# LA Wildfire Prediction: Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.510354Z",
     "start_time": "2025-04-26T20:42:52.502643Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "\n",
    "# For displaying plots in the notebook\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1cc5e768441d71",
   "metadata": {},
   "source": [
    "## Load Model and Feature Names\n",
    "Load the trained model and feature names from saved files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2d15f0344a19c3d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.517046Z",
     "start_time": "2025-04-26T20:42:52.514041Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "  \n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    model = joblib.load(model_path)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84155fe006d32326",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.529508Z",
     "start_time": "2025-04-26T20:42:52.527123Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_feature_names(feature_path):\n",
    "    \n",
    "    print(f\"Loading feature names from {feature_path}...\")\n",
    "    with open(feature_path, 'r') as f:\n",
    "        feature_names = [line.strip() for line in f]\n",
    "    return feature_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b159bac7ab0ba",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "Load the test data and prepare it for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "230a73b464da0b0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.539529Z",
     "start_time": "2025-04-26T20:42:52.535152Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_test_data(data_path, feature_names):\n",
    "  \n",
    "    print(f\"Loading test data from {data_path}...\")\n",
    "    df = pd.read_csv(data_path)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Print the head of loaded data\n",
    "    print(\"\\nLoaded Test Data Head:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Create target variable based on Fire_Occurred\n",
    "    if 'Fire_Occurred' in df.columns:\n",
    "        y_test = df['Fire_Occurred']\n",
    "        print(\"Using Fire_Occurred as target variable for evaluation\")\n",
    "        \n",
    "        # Use the original features for PCA transformation later\n",
    "        X_test = df[feature_names] if all(f in df.columns for f in feature_names) else df[\n",
    "            [f for f in df.columns if f != 'Fire_Occurred' and f != 'date']]\n",
    "        \n",
    "        # Extract dates\n",
    "        date_test = df['date']\n",
    "    else:\n",
    "        # Create a synthetic target if Fire_Occurred is not available\n",
    "        if 'TMAX' in df.columns and 'days_since_rain' in df.columns:\n",
    "            y_test = (df['TMAX'] / 100) * (df['days_since_rain'] / 30)\n",
    "            X_test = df[feature_names] if all(f in df.columns for f in feature_names) else df[\n",
    "                [f for f in df.columns if f != 'date']]\n",
    "            date_test = df['date']\n",
    "            print(\"Created synthetic target based on temperature and dryness for evaluation\")\n",
    "        else:\n",
    "            # Create random target for demonstration\n",
    "            y_test = np.random.rand(len(df))\n",
    "            X_test = df[feature_names] if all(f in df.columns for f in feature_names) else df[\n",
    "                [f for f in df.columns if f != 'date']]\n",
    "            date_test = df['date']\n",
    "            print(\"Created random target for demonstration\")\n",
    "    \n",
    "    return X_test, y_test, date_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae072874c7746d09",
   "metadata": {},
   "source": [
    "## PCA Transformation\n",
    "Apply Principal Component Analysis (PCA) to the test data to match the transformation applied during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dec569564acea940",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.547313Z",
     "start_time": "2025-04-26T20:42:52.544668Z"
    }
   },
   "outputs": [],
   "source": [
    "def apply_pca_to_test_data(X_test, n_components=3):\n",
    "   \n",
    "    print(f\"Applying PCA to test data with {n_components} components...\")\n",
    "    \n",
    "    # Select only numeric columns\n",
    "    numeric_cols = X_test.select_dtypes(include=['float64', 'int64']).columns\n",
    "    X_numeric = X_test[numeric_cols]\n",
    "    \n",
    "    # Create PCA model\n",
    "    pca = PCA(n_components=n_components)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_pca_array = pca.fit_transform(X_numeric)\n",
    "    \n",
    "    # Create principal component column names\n",
    "    component_names = [f'PC{i+1}' for i in range(X_pca_array.shape[1])]\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    X_pca = pd.DataFrame(X_pca_array, columns=component_names)\n",
    "    \n",
    "    print(f\"PCA reduced features from {X_numeric.shape[1]} to {X_pca.shape[1]} components.\")\n",
    "    \n",
    "    return X_pca\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2bc5fcb583b4e",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the model performance using various metrics and visualizations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fd938f3f6176507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.556429Z",
     "start_time": "2025-04-26T20:42:52.552363Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "  \n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate classification metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))\n",
    "    \n",
    "    # Calculate regression metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"\\nMean Squared Error: {mse:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Fire', 'Fire'],\n",
    "                yticklabels=['No Fire', 'Fire'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('../reports/figures', exist_ok=True)\n",
    "    plt.savefig('../reports/figures/confusion_matrix_evaluation.png')\n",
    "    plt.close()\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'f1_score': f1,\n",
    "        'mse': mse,\n",
    "        'r2': r2\n",
    "    }\n",
    "    \n",
    "    # Predict probabilities for the positive class (Fire = 1)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Save true labels, predicted labels, and probabilities\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'True_Label': y_test,\n",
    "        'Predicted_Label': y_pred,\n",
    "        'Predicted_Probability': y_pred_proba\n",
    "    })\n",
    "    os.makedirs('reports', exist_ok=True)\n",
    "    predictions_df.to_csv('reports/predicted_vs_true_labels.csv', index=False)\n",
    "    print(\"Saved predicted vs true labels with probabilities to reports/predicted_vs_true_labels.csv\")\n",
    "\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80163e975932a1",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "Evaluate and visualize the importance of different features in the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "598789841860e650",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.568143Z",
     "start_time": "2025-04-26T20:42:52.563442Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_feature_importance(model, feature_names):\n",
    "    \n",
    "    print(\"Evaluating feature importance...\")\n",
    "    \n",
    "    try:\n",
    "        # For pipeline with classifier step\n",
    "        if hasattr(model, 'named_steps') and hasattr(model.named_steps.get('classifier', None), 'feature_importances_'):\n",
    "            importances = model.named_steps['classifier'].feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        # For direct RandomForestClassifier\n",
    "        elif hasattr(model, 'feature_importances_'):\n",
    "            importances = model.feature_importances_\n",
    "            feature_importance = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importances\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "        else:\n",
    "            print(\"Model does not have feature_importances_ attribute.\")\n",
    "            return None\n",
    "        \n",
    "        # Print top features\n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        # Visualize feature importance\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../reports/figures/feature_importance_evaluation.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Create a pie chart for the top 5 features\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        top5_features = feature_importance.head(5)\n",
    "        plt.pie(top5_features['Importance'], labels=top5_features['Feature'], \n",
    "                autopct='%1.1f%%', startangle=90, shadow=True)\n",
    "        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "        plt.title('Top 5 Features Contribution')\n",
    "        plt.savefig('../reports/figures/top5_features_pie.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return feature_importance\n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating feature importance: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbaad563970bf4",
   "metadata": {},
   "source": [
    "## Seasonal Performance Analysis\n",
    "Evaluate model performance across different seasons to identify seasonal patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b3322143f1add153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.580177Z",
     "start_time": "2025-04-26T20:42:52.573944Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_seasonal_performance(model, data_path, feature_names):\n",
    "    \n",
    "    print(\"Evaluating seasonal performance...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(data_path)\n",
    "    \n",
    "    # Ensure date column is datetime\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # Extract month and season\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['season'] = pd.cut(\n",
    "            df['month'], \n",
    "            bins=[0, 3, 6, 9, 12], \n",
    "            labels=['Winter', 'Spring', 'Summer', 'Fall'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        \n",
    "        # Create target variable for evaluation\n",
    "        if 'Fire_Occurred' in df.columns:\n",
    "            y = df['Fire_Occurred']\n",
    "            \n",
    "            # Check if feature_names are PCA components\n",
    "            if any(name.startswith('PC') for name in feature_names):\n",
    "                # Use all columns except Fire_Occurred and date for PCA\n",
    "                X_features = [col for col in df.columns if \n",
    "                             col != 'Fire_Occurred' and col != 'date' and col != 'month' and col != 'season']\n",
    "                need_pca = True\n",
    "            else:\n",
    "                X_features = feature_names\n",
    "                need_pca = False\n",
    "        else:\n",
    "            # Create a synthetic target\n",
    "            if 'TMAX' in df.columns and 'days_since_rain' in df.columns:\n",
    "                y = (df['TMAX'] / 100) * (df['days_since_rain'] / 30)\n",
    "                X_features = feature_names\n",
    "                need_pca = any(name.startswith('PC') for name in feature_names)\n",
    "            else:\n",
    "                # Create random target for demonstration\n",
    "                y = np.random.rand(len(df))\n",
    "                X_features = feature_names\n",
    "                need_pca = any(name.startswith('PC') for name in feature_names)\n",
    "        \n",
    "        # Evaluate by season\n",
    "        seasons = df['season'].unique()\n",
    "        seasonal_metrics = []\n",
    "        \n",
    "        for season in seasons:\n",
    "            season_df = df[df['season'] == season]\n",
    "            X_season = season_df[X_features]\n",
    "            y_season = y[season_df.index]\n",
    "            \n",
    "            # Apply PCA if needed\n",
    "            if need_pca:\n",
    "                n_components = sum(1 for name in feature_names if name.startswith('PC'))\n",
    "                X_season = apply_pca_to_test_data(X_season, n_components)\n",
    "            \n",
    "            # Make predictions\n",
    "            try:\n",
    "                y_pred = model.predict(X_season)\n",
    "                \n",
    "                # Calculate metrics\n",
    "                acc = accuracy_score(y_season, y_pred)\n",
    "                f1 = f1_score(y_season, y_pred)\n",
    "                \n",
    "                # Add to results\n",
    "                seasonal_metrics.append({\n",
    "                    'Season': season,\n",
    "                    'Accuracy': acc,\n",
    "                    'F1 Score': f1,\n",
    "                    'Sample Size': len(season_df)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating season {season}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        seasonal_df = pd.DataFrame(seasonal_metrics)\n",
    "        \n",
    "        # Display results\n",
    "        print(\"\\nSeasonal Performance:\")\n",
    "        print(seasonal_df)\n",
    "        \n",
    "        # Visualize seasonal performance\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        seasonal_df.set_index('Season')[['Accuracy', 'F1 Score']].plot(kind='bar')\n",
    "        plt.title('Model Performance by Season')\n",
    "        plt.ylabel('Score')\n",
    "        plt.savefig('../reports/figures/seasonal_performance.png')\n",
    "        plt.close()\n",
    "        \n",
    "        return seasonal_df\n",
    "    else:\n",
    "        print(\"Date column not found. Cannot evaluate seasonal performance.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c76b0a36b4fac0d",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "Run the complete model evaluation pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "89296c64647a912f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:42:52.588850Z",
     "start_time": "2025-04-26T20:42:52.585486Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \"\"\"Main function for evaluating the model.\"\"\"\n",
    "    # Define file paths\n",
    "    model_path = \"../models/la_fire_random_forest_model.pkl\"\n",
    "    feature_path = \"../models/feature_names.txt\"\n",
    "    data_path = \"../data/processed/engineered_la_fire_data.csv\"\n",
    "    \n",
    "    # Create reports directory if it doesn't exist\n",
    "    os.makedirs('../reports/figures', exist_ok=True)\n",
    "    \n",
    "    # Load model and feature names\n",
    "    model = load_model(model_path)\n",
    "    feature_names = load_feature_names(feature_path)\n",
    "    \n",
    "    # Load test data\n",
    "    X_test, y_test, date_test = load_test_data(data_path, feature_names)\n",
    "    \n",
    "    # Check if we need to apply PCA\n",
    "    if any(name.startswith('PC') for name in feature_names):\n",
    "        # Count the number of PC components\n",
    "        n_components = sum(1 for name in feature_names if name.startswith('PC'))\n",
    "        # Apply PCA to test data\n",
    "        X_test = apply_pca_to_test_data(X_test, n_components)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate_model(model, X_test, y_test)\n",
    "    \n",
    "    # Evaluate feature importance\n",
    "    feature_importance = evaluate_feature_importance(model, feature_names)\n",
    "    \n",
    "    # Evaluate seasonal performance\n",
    "    seasonal_metrics = evaluate_seasonal_performance(model, data_path, feature_names)\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        'metrics': metrics,\n",
    "        'top_features': feature_importance.head(10).to_dict() if feature_importance is not None else None,\n",
    "        'seasonal_performance': seasonal_metrics.to_dict() if seasonal_metrics is not None else None\n",
    "    }\n",
    "    \n",
    "    # Save as JSON\n",
    "    import json\n",
    "    with open('../reports/evaluation_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    print(\"\\nEvaluation results saved to reports/evaluation_results.json\")\n",
    "    print(\"Model evaluation completed successfully!\")\n",
    "    \n",
    "    return metrics, feature_importance, seasonal_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "584dff78c460413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T20:43:05.796077Z",
     "start_time": "2025-04-26T20:42:52.593970Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ../models/la_fire_random_forest_model.pkl...\n",
      "Loading feature names from ../models/feature_names.txt...\n",
      "Loading test data from ../data/processed/engineered_la_fire_data.csv...\n",
      "\n",
      "Loaded Test Data Head:\n",
      "        date  Fire_Occurred STATION NAME  AWND  DAPR  MDPR  PGTM  PRCP  TAVG  \\\n",
      "0 2014-12-27              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1 2014-12-28              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2 2014-12-29              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "3 2014-12-30              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4 2014-12-31              0       0    0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "\n",
      "   ...  season_Summer  season_Fall  TMAX_3D  TMIN_3D  PRCP_3D  PRCP_14D  \\\n",
      "0  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "1  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "2  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "3  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "4  ...          False         True      0.0      0.0      0.0       0.0   \n",
      "\n",
      "   AWND_3D  days_since_rain  drought_index  fire_spread_potential  \n",
      "0      0.0              1.0            0.0                   0.01  \n",
      "1      0.0              2.0            0.0                   0.02  \n",
      "2      0.0              3.0            0.0                   0.03  \n",
      "3      0.0              4.0            0.0                   0.04  \n",
      "4      0.0              5.0            0.0                   0.05  \n",
      "\n",
      "[5 rows x 49 columns]\n",
      "Using Fire_Occurred as target variable for evaluation\n",
      "Applying PCA to test data with 3 components...\n",
      "PCA reduced features from 42 to 3 components.\n",
      "Evaluating model performance...\n",
      "Accuracy: 0.9355\n",
      "F1 Score: 0.0000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97    210508\n",
      "           1       0.00      0.00      0.00     14478\n",
      "\n",
      "    accuracy                           0.94    224986\n",
      "   macro avg       0.47      0.50      0.48    224986\n",
      "weighted avg       0.88      0.94      0.90    224986\n",
      "\n",
      "\n",
      "Mean Squared Error: 0.0645\n",
      "R² Score: -0.0705\n",
      "Saved predicted vs true labels with probabilities to reports/predicted_vs_true_labels.csv\n",
      "Evaluating feature importance...\n",
      "\n",
      "Top 10 Most Important Features:\n",
      "  Feature  Importance\n",
      "0     PC1    0.464723\n",
      "1     PC2    0.282216\n",
      "2     PC3    0.253061\n",
      "Evaluating seasonal performance...\n",
      "Applying PCA to test data with 3 components...\n",
      "PCA reduced features from 41 to 3 components.\n",
      "Applying PCA to test data with 3 components...\n",
      "PCA reduced features from 41 to 3 components.\n",
      "Applying PCA to test data with 3 components...\n",
      "PCA reduced features from 41 to 3 components.\n",
      "Applying PCA to test data with 3 components...\n",
      "PCA reduced features from 41 to 3 components.\n",
      "\n",
      "Seasonal Performance:\n",
      "   Season  Accuracy  F1 Score  Sample Size\n",
      "0    Fall  0.937683  0.000000        56405\n",
      "1  Winter  0.999738  0.000000        57173\n",
      "2  Spring  0.911293  0.017765        56095\n",
      "3  Summer  0.878998  0.000000        55313\n",
      "\n",
      "Evaluation results saved to reports/evaluation_results.json\n",
      "Model evaluation completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Execute the model evaluation pipeline\n",
    "metrics, feature_importance, seasonal_metrics = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80a9448-fede-4255-9831-1d0db6b8d870",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
